---
layout: post
title: 机器学习面试003——正则化
categories: [机器学习, 面试]
description: 面试总结
keywords: ML, L1, L2, 正则
---

### 1. L1范数和L2范数的区别是什么？
<img src="/images/machine learning/004-Norm-01.png" width="80%" alt="gecko embed program run error 2" />

**Ans**：①**L1范数**——指向量中各个元素的绝对值之和，又叫“稀疏规则算子”(Lasso regularization)。它可以实现**特征的自动选择**，一般，大部分特征*x*和*y*没有多大关系，在最小化目标函数时，考虑这些额外的特征虽然能减少训练误差，但是在预测新样本时，会干扰模型对正确结果的预测。L1算子可以学习去掉这些没有信息的特征，让其对应的权重为0。
②**L2范数**——在回归里面，又称“岭回归”(Ridge Regression)，有时也被称为“权值衰减”(weight decay)。它可以解决过拟合，使得*w*的每个元素都很小(接近0)，但不会置为0.
③加入正则相当于加入了一种**先验**。**L1相当于加入了Laplacean先验；L2相当于加入了Gaussian先验。**

### 2. 机器学习中，为何要常对数据进行归一化？
#### 1.归一化能够提高梯度下降的最优解求解速度。
> 详细参考斯坦福视频：https://class.coursera.org/ml-003/lecture/21

<img src="/images/machine learning/004-Norm-02.png" width="80%" alt="gecko embed program run error 2" />
> 如上图所示，蓝色线代表特征等高线，X1和X2的特征区间相差很大，当使用梯度下降法求解时，很可能走“之字型”路线（垂直等高线），从而需要迭代很多次才能收敛；

> 归一化后，等高线显得很圆，梯度下降能很快收敛。

#### 2. 归一化，有可能提高精度
> 一些分类器需要计算样本之间的距离（如kNN中的欧式距离）。如果一个特征值范围非常大，那么距离计算就主要取决于这个特征。

+ 线性归一
> x = x - min(x) / max(x) - min(x)

+ 标准化归一
> x = x - μ / σ

+ 非线性归一化
> 经常用在数据分化比较大的情况，如log2，log10

### 哪些机器学习算法不需要做归一化
> 概率模型（或树形模型），如决策树，随机森林

#### 为什么树形结构不需要归一化？
> 数值缩放，不影响分裂点的位置。

因为第一步都是按照特征值进行排序，排序不变，所属的分支和分裂点就不会不同。
> 一般树形结构不能进行梯度下降

因为树模型是阶跃的，阶跃点不可导，所以树模型寻找最优点是通过寻找最优分裂点完成的。
